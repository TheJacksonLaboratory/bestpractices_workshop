[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Best Practices in Python Workshop",
    "section": "",
    "text": "Best Practices in Python Workshop\nIn this workshop, we will cover a few tools to improve Python code readability, maintainability, and reproducibility.\nPre-Requisites: Novice to intermediate Python knowledge, users who would like to improve their skills and write cleaner, more robust code.\nLessons:\n\nWorking with virtual environments and conda\nTesting Python code with pytest\nWriting well-documented and clean code: docstrings and linting\nPython type hints\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "pytest.html",
    "href": "pytest.html",
    "title": "Testing Python code with pytest",
    "section": "",
    "text": "In this section, we are going to be using pytest to run automated tests on some code. The code we are going to be using is in the arrays folder within this repository. The functions that will be tested are in arrays.py, and the test code will go in test_arrays.py. Testing your code is extremely important, and it should be done WHILE you are writing it, rather than AFTER.\nUsual methods for testing code are doing some manual checks, such as running it over particular input files or variables and checking the results. This has limitations: it might fail to check some parts of the code, or it might fail to find errors that are not immediately obvious. In either case, it is also difficult to find exactly where your errors might be.\nTo avoid those pitfalls, we should write a set of tests that use known inputs and check for matching with a set of expected outputs. We will write each test to run over as little of the code as possible, such that we can easily identify which parts of the code are failing. This type of test is called a “unit test”, in contrast to “integration tests” that test multiple parts of the code at once. Tip: break long, complex functions with multiple control structures (e.g. if/else, for, while) into smaller functions that do one thing each to make your code easier to read, test, and maintain.\nLet’s start by looking into arrays.py and checking the add_arrays() function. It’s a pretty simple function; it takes two arrays and add them up element-wise. Now, let’s look at test_add_arrays() in test_arrays.py. How is testing being done here? Can you break it? What happens when you run:\npython arrays/test_arrays.py\n(Is your virtual environment active? If not, activate it first!)\nThe output of this test is not particularly useful. Imagine if you had five different functions with five different tests; would an output like OK BROKEN OK BROKEN BROKEN help much?\nInstead of that structure, we are going to use assert statements; assert is always followed by something boolean (i.e. something that will be either true or false). Empty lists, the number 0 and None are all false-y. If the boolean is true, nothing happens when assert is run; if it is false, an exception is raised. You can try running assert 5 == 5 and assert 5 == 6 in a Python shell to see what happens.\nNow we are going to replace the if/else block in test_add_arrays() with an assert that looks like assert output == expect. What happens when you run python arrays/test_arrays.py? What if the test fails?\nYou see that now at least we get a specific line when the test fails; that’s a good start! However, if we had multiple tests, code execution would be stopped at the exception thrown by assert. Also, we still need to explicitly call test_add_arrays() in that test file, which would be easy to forget, especially if we had a bunch of tests. That’s where we are going to be using pytest!\nOur first step is removing the call to test_add_arrays() from the end of test_arrays.py; pytest will take care of that for us. Now, in your terminal, just run pytest. What happened? What if the test fails?\npytest will find all files named test_*.py and *_test.py and all functions starting with names starting with test inside these files, and it will run those, one at a time, reporting the results of each.\nExercise: It’s your turn to write a test! Write a test_subtract_arrays() function in test_arrays.py that tests the subtract_arrays() function in arrays.py! What happens when you run pytest now?\nExercise: Let’s do the opposite: write a test_multiply_arrays() function with the behavior you would expect to see from a multiply_arrays() function. Then, write the multiply_arrays() to fulfill the test requirements.\nThis is a process called Test-driven development (TDD): you start by writing your code requirements as tests and then write code that passes those tests. It is a popular approach in certain kinds of software development.\nNow imagine you want to test multiple cases in test_add_arrays(): positive results, negative results, zero results, etc. You could change the code in that test function to create the a, b and expect arrays multiple times, and do one assertion per case.\nHowever, pytest allows for a simpler possibility: parameterizing inputs to your test. You can do this using the decorator @pytest.mark.parametrize() before your test function.\nIt takes two arguments: a tuple or string with the names of the parameters you want to pass to this function, and a list containing tuples of values of the parameters you want to pass. So for a single case, it would look like @pytest.mark.parametrize((\"a\", \"b\", \"expect\"), [([1, 2, 3], [4, 5, 6], [5, 7, 9])]), and you could add extra tuples for additional test cases. Then all you need to do is add a, b and expect as arguments to your test function.\nExercise: Try using @pytest.mark.parametrize() for your test functions. What happens when you run pytest now? Write a test for divide_arrays() using this approach. Can you find the bugs in the divide_arrays() function with some clever testing?\nSo far, we have assumed that everything passed to our array functions is correct; that is rarely an assumption you can make in real life. What happens if you run add_arrays(\"this is a string\", 1)? What about add_arrays([1, 2], [1, 2, 3])?\nIt’s time to add explicit exception handling to our functions. You will probably want to do raise ValueError(\"array size mismatch\") for the case where arrays sizes are different, and raise TypeError(\"arguments should be lists\") for when the arguments are not lists.\nNow, we can add new test functions named test_add_arrays_error() and so on, where we check if errors are being raised correctly. That is done by wrapping our function call with with pytest.raises(ValueError), for example. What happens when you run pytest now? Add checks for both possible errors we came up with. Better yet, use the match keyword argument to check the string and ensure the right ValueError was raised! What other cases can happen in e.g. divide_arrays()?\nWe have successfully found a way to separate the data to be tested from the code to be tested. pytest has an even better way to do that for more complex cases, for example, when you want multiple test functions using the same data. They are called fixtures.\nA fixture is defined as a function that returns something we want to use repeatedly in our tests. pytest provides some fixtures out of the box, like the very useful tmp_path fixture that gives you a unique temporary location.\nBut you can also create your own: fixtures can be used to set up test data, create mock objects, or perform any other setup tasks that are needed for your tests. To define a fixture, you use the decorator @pytest.fixture before a function. After defining a fixture, you can pass the name of the function as an argument in your test function, and that argument will assume the value that is returned by the fixture.\nTry creating a pair_of_lists() fixture and passing it to a test function. What happens when you run pytest? is pair_of_lists() run?\nSome final tips: append -v for more verbose output or -s to see print() outputs. You can also run specific tests by passing the file name and function name to pytest, e.g. pytest arrays/test_arrays.py::test_add_arrays.\n\n\n\n\nWhen writing tests, it’s important to know how much of your code is actually being tested by your test suite. This is called code coverage. Code coverage is a metric that tells you what percentage of your code is run (“covered”) when your tests are executed. High coverage means most of your code is tested, while low coverage means there are many untested parts, where bugs could hide.\nThe most common tool for measuring code coverage in Python is coverage. You can run it from the command line to see how much of your code is covered by your tests.\nTo use coverage with pytest, run:\ncoverage run -m pytest\nThis will run your tests and collect coverage data.\nTo see a summary in your terminal, run:\ncoverage report\nTo generate a detailed HTML report you can view in your browser, run:\ncoverage html\nThen open the file htmlcov/index.html in your browser to explore which lines of code are covered and which are not.\nReviewing your coverage report can help you identify untested code and improve your test suite.\n\n\n\n\n\npytest offers extensive features to make testing your code easier, so check out the pytest documentation.\nThe unittest.mock module is also very useful for testing more complex code. It allows you to create mock objects needed by your code and then make assertions about how they have been used. You can even “patch” objects with your mocks. When unit testing, the idea is to test only the code that you own. Mocks are particularly useful for testing code that interacts with external systems, such as databases or web services.\nThere is also the pytest “monkeypatching” fixture, which allows you to safely set/delete an attribute, dictionary item or environment variable, or even modify sys.path for importing.\npytest has an extensive ecosystem of plugins that can help you with specific testing needs. For example, there are plugins for testing web applications, databases, and more. You can find a list of available plugins on the pytest website.\nFinally, you can consider using hypothesis, a property-based testing library that will generate random input data based on specified properties. This can help you find edge cases and unexpected behavior in your code that you might not have thought of when writing traditional unit tests.\n\n\n\n\n\nIf you’re using GitHub for your code repositories, you can set up automated testing so that your tests are run automatically whenever you push new code or open a pull request. This is done using GitHub Actions, which allows you to define workflows that run on specific events.\nLet’s look at the file at .github/workflow/run_tests.yml. This is a Github Action file - it will specify actions that will happen on Github when you do some things on your repository. What is happening here? When it is triggered? Try using a file like this in one of your repositories (if you have one) or a fork of this repository, pushing a new commit to Github and checking the “actions” tab on your repository’s page.\nFinally, try adding code coverage to your Github action. Did it work when a new commit was pushed? What was produced?",
    "crumbs": [
      "Testing"
    ]
  },
  {
    "objectID": "pytest.html#test-coverage",
    "href": "pytest.html#test-coverage",
    "title": "Testing Python code with pytest",
    "section": "",
    "text": "When writing tests, it’s important to know how much of your code is actually being tested by your test suite. This is called code coverage. Code coverage is a metric that tells you what percentage of your code is run (“covered”) when your tests are executed. High coverage means most of your code is tested, while low coverage means there are many untested parts, where bugs could hide.\nThe most common tool for measuring code coverage in Python is coverage. You can run it from the command line to see how much of your code is covered by your tests.\nTo use coverage with pytest, run:\ncoverage run -m pytest\nThis will run your tests and collect coverage data.\nTo see a summary in your terminal, run:\ncoverage report\nTo generate a detailed HTML report you can view in your browser, run:\ncoverage html\nThen open the file htmlcov/index.html in your browser to explore which lines of code are covered and which are not.\nReviewing your coverage report can help you identify untested code and improve your test suite.",
    "crumbs": [
      "Testing"
    ]
  },
  {
    "objectID": "pytest.html#more-advanced-testing",
    "href": "pytest.html#more-advanced-testing",
    "title": "Testing Python code with pytest",
    "section": "",
    "text": "pytest offers extensive features to make testing your code easier, so check out the pytest documentation.\nThe unittest.mock module is also very useful for testing more complex code. It allows you to create mock objects needed by your code and then make assertions about how they have been used. You can even “patch” objects with your mocks. When unit testing, the idea is to test only the code that you own. Mocks are particularly useful for testing code that interacts with external systems, such as databases or web services.\nThere is also the pytest “monkeypatching” fixture, which allows you to safely set/delete an attribute, dictionary item or environment variable, or even modify sys.path for importing.\npytest has an extensive ecosystem of plugins that can help you with specific testing needs. For example, there are plugins for testing web applications, databases, and more. You can find a list of available plugins on the pytest website.\nFinally, you can consider using hypothesis, a property-based testing library that will generate random input data based on specified properties. This can help you find edge cases and unexpected behavior in your code that you might not have thought of when writing traditional unit tests.",
    "crumbs": [
      "Testing"
    ]
  },
  {
    "objectID": "pytest.html#automated-testing",
    "href": "pytest.html#automated-testing",
    "title": "Testing Python code with pytest",
    "section": "",
    "text": "If you’re using GitHub for your code repositories, you can set up automated testing so that your tests are run automatically whenever you push new code or open a pull request. This is done using GitHub Actions, which allows you to define workflows that run on specific events.\nLet’s look at the file at .github/workflow/run_tests.yml. This is a Github Action file - it will specify actions that will happen on Github when you do some things on your repository. What is happening here? When it is triggered? Try using a file like this in one of your repositories (if you have one) or a fork of this repository, pushing a new commit to Github and checking the “actions” tab on your repository’s page.\nFinally, try adding code coverage to your Github action. Did it work when a new commit was pushed? What was produced?",
    "crumbs": [
      "Testing"
    ]
  },
  {
    "objectID": "docstrings.html",
    "href": "docstrings.html",
    "title": "Writing well-documented and clean code",
    "section": "",
    "text": "In this section, we are going to be looking at docstrings. Docstrings are an easy way to write reference documentation that is easy to read for both humans and computers. It includes comprehensive information about what your code does and how it does it, and it can be easily reused as part of computer-generated documentation packages such as sphinx. For more details, see Python Enhancement Proposal 257, PEP 257.\nFor this guide, we will be following numpy’s style guide when it comes to docstrings. This guide has been adopted by a large portion of scientific Python packages, so if you decide to follow these guidelines you will quickly find that they will look like a lot of the the documentation on numpy, or pandas, or many other very popular packages. Also, many automated documentation generators work very well with this format.\nDocstrings are strings that describe a module, function or class. They can be directly accessed in Python (object.__doc__). For consistency purposes, we will always surround them with a triple double quote (\"\"\").\nA good docstring should start with a one-line summary of what the object or function does. Try not to use any variable names or the name of the function in it to avoid redundancy!\nAfter that, it’s a good idea to have a next section with a few sentences describing your function/module/class in more detail. The idea here is to clarify functionality rather than discuss implementation details. Feel free to refer to parameter and function names, but you don’t need to go into too much detail here; we will have a separate section describing the parameters.\nNext, we should describe the function arguments and keywords. Make sure to mention their types and what each of them mean. Variable names should be enclosed in single backticks; that will ensure it will show up as code when using automated documentation generators. Be as precise as possible when it comes to variable types and make sure to note if a specific argument is optional.\nNow, do the same for the return values of your function. If your function yields any values, then do the same for any yielded values. If your function raises any exceptions or warnings, make sure to have a Raises and/or Warns section as well.\nIf you would like to describe implementation details, this is a good place to put a Notes section. If what you’re doing includes equations, you can write them in LaTeX format. You can also add a References section if you would like to point at specific papers where the implementation came from, for example.\nIt’s good practice to include an Examples section as well - this is just to show how your code would be used, not for testing! If your function has optional arguments or can be used in multiple ways, make sure to include multiple examples.\nNow, put all the information you got together and write docstrings for the functions we have in arrays.py! As an example, I am adding an example docstring for add_arrays() below:\ndef add_arrays(x, y):\n    \"\"\"This function adds together each element of the two passed lists.\n\n    Parameters\n    ----------\n    x : list\n        The first list to add.\n    y : list\n        The second list to add.\n\n    Returns\n    -------\n    z : list\n        The pairwise sums of ``x`` and ``y``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; add_arrays([1, 4, 5], [4, 3, 5])\n    [5, 7, 10]\n\n    \"\"\"\nNote the underscores and line spacing in the sections; those are part of the numpy docstring style!\n\n\n\n\n\nWhile it’s important to write code that works, it’s also important to write code that can be easily maintained and understood. Towards those aims, there are a set of guidelines outlined in PEP 8 for the style and layout of Python code, to improve readability and follow general conventions. Some specific software projects have their own style guidelines; the PEP 8 guidelines are very general ones.\nThere are A LOT of guidelines. I have not memorized them, and I do not expect anyone will. I have probably never run into many of them. So how do you make sure you are following them? Well, there are many tools for automatically checking your code against the guidelines; this is called “linting”.\nMy go-to is Ruff, a fast Python linter (and formatter) that is a newer alternative to tools like flake8 (and black). You should already have it installed on your conda environment.\nGo ahead and run ruff check in your repository directory and see if it returns anything. In general, the warnings raised by Ruff are pretty self-explanatory (“No newline at end of file”, “Blank line contains whitespace”), but if you have something flagged that you’re not sure about, you can see the full list of rules. If you click on the name of a rule, you will get a nice explanation.\nIn general, it is a good idea to do a Ruff pass on your code before committing or merging it! Tip: you can also run ruff check --fix to automatically fix some of the issues it finds.\nYou can specify extra rules using --select, for example ruff check --select D will only check for docstring related issues, while I will look at import sorting. You can also ignore specific rules using --ignore, for example ruff check --ignore E501 will ignore line length issues.\nFinally, you can also use ruff as a formatting tool, similar to black. You can run ruff format on your repository directory to automatically reformat your code to follow PEP 8 guidelines. This is a great way to ensure that your code is consistently formatted and adheres to best practices. This can make it easier to avoid merge conflicts due to formatting/whitespace differences, however if you are working on a shared codebase, make sure to check the contribution guide before running any autoformatting tools.",
    "crumbs": [
      "Docstrings & Linting"
    ]
  },
  {
    "objectID": "docstrings.html#docstrings",
    "href": "docstrings.html#docstrings",
    "title": "Writing well-documented and clean code",
    "section": "",
    "text": "In this section, we are going to be looking at docstrings. Docstrings are an easy way to write reference documentation that is easy to read for both humans and computers. It includes comprehensive information about what your code does and how it does it, and it can be easily reused as part of computer-generated documentation packages such as sphinx. For more details, see Python Enhancement Proposal 257, PEP 257.\nFor this guide, we will be following numpy’s style guide when it comes to docstrings. This guide has been adopted by a large portion of scientific Python packages, so if you decide to follow these guidelines you will quickly find that they will look like a lot of the the documentation on numpy, or pandas, or many other very popular packages. Also, many automated documentation generators work very well with this format.\nDocstrings are strings that describe a module, function or class. They can be directly accessed in Python (object.__doc__). For consistency purposes, we will always surround them with a triple double quote (\"\"\").\nA good docstring should start with a one-line summary of what the object or function does. Try not to use any variable names or the name of the function in it to avoid redundancy!\nAfter that, it’s a good idea to have a next section with a few sentences describing your function/module/class in more detail. The idea here is to clarify functionality rather than discuss implementation details. Feel free to refer to parameter and function names, but you don’t need to go into too much detail here; we will have a separate section describing the parameters.\nNext, we should describe the function arguments and keywords. Make sure to mention their types and what each of them mean. Variable names should be enclosed in single backticks; that will ensure it will show up as code when using automated documentation generators. Be as precise as possible when it comes to variable types and make sure to note if a specific argument is optional.\nNow, do the same for the return values of your function. If your function yields any values, then do the same for any yielded values. If your function raises any exceptions or warnings, make sure to have a Raises and/or Warns section as well.\nIf you would like to describe implementation details, this is a good place to put a Notes section. If what you’re doing includes equations, you can write them in LaTeX format. You can also add a References section if you would like to point at specific papers where the implementation came from, for example.\nIt’s good practice to include an Examples section as well - this is just to show how your code would be used, not for testing! If your function has optional arguments or can be used in multiple ways, make sure to include multiple examples.\nNow, put all the information you got together and write docstrings for the functions we have in arrays.py! As an example, I am adding an example docstring for add_arrays() below:\ndef add_arrays(x, y):\n    \"\"\"This function adds together each element of the two passed lists.\n\n    Parameters\n    ----------\n    x : list\n        The first list to add.\n    y : list\n        The second list to add.\n\n    Returns\n    -------\n    z : list\n        The pairwise sums of ``x`` and ``y``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; add_arrays([1, 4, 5], [4, 3, 5])\n    [5, 7, 10]\n\n    \"\"\"\nNote the underscores and line spacing in the sections; those are part of the numpy docstring style!",
    "crumbs": [
      "Docstrings & Linting"
    ]
  },
  {
    "objectID": "docstrings.html#linting",
    "href": "docstrings.html#linting",
    "title": "Writing well-documented and clean code",
    "section": "",
    "text": "While it’s important to write code that works, it’s also important to write code that can be easily maintained and understood. Towards those aims, there are a set of guidelines outlined in PEP 8 for the style and layout of Python code, to improve readability and follow general conventions. Some specific software projects have their own style guidelines; the PEP 8 guidelines are very general ones.\nThere are A LOT of guidelines. I have not memorized them, and I do not expect anyone will. I have probably never run into many of them. So how do you make sure you are following them? Well, there are many tools for automatically checking your code against the guidelines; this is called “linting”.\nMy go-to is Ruff, a fast Python linter (and formatter) that is a newer alternative to tools like flake8 (and black). You should already have it installed on your conda environment.\nGo ahead and run ruff check in your repository directory and see if it returns anything. In general, the warnings raised by Ruff are pretty self-explanatory (“No newline at end of file”, “Blank line contains whitespace”), but if you have something flagged that you’re not sure about, you can see the full list of rules. If you click on the name of a rule, you will get a nice explanation.\nIn general, it is a good idea to do a Ruff pass on your code before committing or merging it! Tip: you can also run ruff check --fix to automatically fix some of the issues it finds.\nYou can specify extra rules using --select, for example ruff check --select D will only check for docstring related issues, while I will look at import sorting. You can also ignore specific rules using --ignore, for example ruff check --ignore E501 will ignore line length issues.\nFinally, you can also use ruff as a formatting tool, similar to black. You can run ruff format on your repository directory to automatically reformat your code to follow PEP 8 guidelines. This is a great way to ensure that your code is consistently formatted and adheres to best practices. This can make it easier to avoid merge conflicts due to formatting/whitespace differences, however if you are working on a shared codebase, make sure to check the contribution guide before running any autoformatting tools.",
    "crumbs": [
      "Docstrings & Linting"
    ]
  },
  {
    "objectID": "typing.html",
    "href": "typing.html",
    "title": "Python type hints",
    "section": "",
    "text": "Python type hints\n\nIn this section, we will look at type hints. Type hints are a way to annotate your Python code with information about the expected types of variables, function arguments, and return values.\nPython is dynamically typed, so this is not required. However, type hints help both humans and tools understand your code, which can catch bugs early and improve code readability and maintainability.\nType hints are part of the Python standard library since Python 3.5. For more details, see PEP484 which introduced type hints and the typing module documentation. Importantly, type hints do not change the runtime behavior of your code; they are simply annotations.\nWhy use type hints?\n\nClarity: Type hints make it clear what types your functions expect and return.\nEarly error detection: Tools like mypy can check your code for type errors before you run it.\nBetter IDE support: Many editors use type hints for autocompletion and inline documentation.\n\nType hints use the : syntax for variables and function arguments, and -&gt; for return types. For example:\n# Variable annotation\na: int = 5\n\n# Function annotation\ndef add(x: int, y: int) -&gt; int:\n    return x + y\nExamples of common types you may use in Python 3.9 and later:\n\nint, float, str, bool: Basic types\nlist, dict, set, tuple: Collection types.\n\nThese can be combined using bracket notation like: list[int], dict[str, float], etc.\nYou can also import additional types from the typing module or other modules you use\nType hints are not enforced at runtime, but you can use tools like mypy to check your code for type errors. To check your typed code, run:\nmypy your_script.py\nmypy will report any type mismatches it finds, helping you catch bugs before they happen.\nIf you want to learn more, check out:\n\nPython typing documentation\nmypy Type hints cheat sheet\n\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "Type Hints"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "Github workflow: branch-update-PR",
    "section": "",
    "text": "Github workflow: branch-update-PR\n\nWe are going to use Github Desktop for these tasks here. If you are comfortable with command-line git, feel free to use that instead.\nThe first step is to go to the workshop repository and fork it. This will create a new repository under your user account that is a “mirror” of the original workshop repository.\n\nThis way, you can work “locally” on your own repository, in your own branch, and when changes are ready to be submitted to the “gold standard” repository you can do that. Right now you should have something like this:\n\nThe next step is to create a new branch. Branches are what they sound like: from a codebase, you can “branch” out to try something new, and when that is done you can merge the changes back to the main source code. Even when working on your own fork, it is a good idea to create a new branch for changes you are making.\nIn this exercise, the only change we are going to make is adding your name to the list of participants on README.md. First off, we are going to clone your fork of the workshop repository to your computer. If you had already cloned the workshop repository with Github Desktop, remove it and make sure to clone your fork.\nGo to File -&gt; Clone repository… and then search for “bestpractices”. You should see two of them: the original workshop repo belonging to TheJacksonLaboratory and your fork. Make sure to clone your own fork.\n\nYou will be asked if you want to plan to use this fork “to contribute to the parent project” or “for my own purposes”. This is largely what it sounds like: choosing the first one allows you to suggest changes to the parent repository, while the second one would change only your own version of it. Since we want to create a single list of all participants, choose “to contribute to the parent project”.\nYou now have a local version of the repository on your computer to play with! You can have a look at it by going to Repository -&gt; Show in Explorer.\nOur next step is to create a branch where we will make the change to the list of participants. Go to Branch -&gt; New branch. Give it an informative name that describes what the change you’ll be doing is and then click “create branch”:\n\nYou now should see that your current branch is the one you just created. Right now, it only exists in your computer; to push the new branch to Github, click “Publish Branch”:\n\nIf you go to your forked repository on the Github webpage, you can now see that you have two branches:\n\nNow, we’ll make the changes we want to make to our branch in our fork. Open README.md from the repository folder you have on your computer and add your name to the list of participants. After you save the file, you should see the list of changes on your Github Desktop window:\n\nNow we’ll commit these changes. Give the commit a good description that summarizes the change and click “commit to YOURBRANCHNAMEHERE”.\n\nThis commit is still stored locally, so make sure to push it to the remote version of your repository hosted on github.\n\nGithub Desktop should now give you the option to create a pull request from your current branch. A pull request can be thought of as a “request to change”; you submit your proposed changes to the original repository, and the maintainers of that repository can choose whether to make them the new “standard” or not. Click “Create Pull Request”.\n\nThis should open a browser window that will take you to the Github website. It will show you from which branch to which branch that pull request is being created (from your fork and your branch, to the original repository and “main” branch). Here, you have a chance to write a title and a description to your PR. It’s a good idea to make them as descriptive as possible, to make it clear for the maintainers what exactly is being changed and why. In this case I will be pretty minimalistic, but you shouldn’t!\n\nYou can see that you’re taken to the main repository belonging to TheJacksonLaboratory, and there is now an open pull request. If you are not a repository maintainer, your work is more or less done at this point – the maintainers will check your PR, and might request changes. When they are happy with those, they will merge your pull request, and it will become the new standard! If you are a maintainer, you might face the dreaded merge conflicts here – sometimes two versions of the same codebase cannot be seamlessly merged and you will need to solve the conflicts by picking which version should take precedence. We will NOT cover these here!\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "conda.html",
    "href": "conda.html",
    "title": "Working with virtual environments and conda",
    "section": "",
    "text": "To work with Python projects, it’s a good idea to use virtual environments. Virtual environments allow you to create isolated spaces for your Python projects, each with its own set of packages and dependencies. This way, you can avoid conflicts between different projects that may require different versions of the same package. This is particularly important in scientific computing, where you may have a long-running project that you need to keep working with, while also wanting to be able to start something new with its own dependencies.\nAt the heart of a Python environment are:\n\nA Python interpreter: a specific version of Python that will be used to execute your code\nAn environment management tool: a tool to create, manage, and switch between different environments\nA package management tool: a tool to install, update, and remove packages within an environment\n\nConda is a tool that combines all three of these functionalities. It allows you to create and manage virtual environments, manage different versions of Python, and install packages. Conda is particularly popular in the scientific Python community due to its ability to handle complex system-level dependencies, such as GPU support, and its support for packages outside of the Python ecosystem (such as R, C libraries, etc).\nThere are other tools that can be used to manage virtual environments, such as venv (built into Python) and newer ones like uv and pixi. These use a project oriented approach, where the environment is tied to a specific project directory. Conda, on the other hand creates and manages environments centrally, independently of any specific directory. At the end of the day, it’s a matter of preference and use case, but the key take home message is to actually always use virtual environments for your projects!\n\n\n\n\nThere are several distributions of conda and it can pull packages from many different sources called “channels”. In this workshop we will use the conda-forge distribution called “miniforge” (or “miniforge3”).\n\nGo to the conda-forge downloads page: https://conda-forge.org/download/.\n\nDownload the installer for your operating system (Mac, Windows, Linux) and architecture (x86_64/amd64 or arm64/aarch64) and follow the installation instructions on that page.\n\nNote: When prompted whether to “automatically initialize conda”, I recommend saying “yes”. And on Windows, I recommend checking the options to “Create start menu shortcuts” and “Add Miniforge3 to my PATH environment variable”.\n\nOpen a NEW terminal window. If you see (base) at the beginning of your prompt, you have installed it successfully. You can also run conda --version and see if you get a version output.\n\n\n\n\n\nThe (base) at the beginning of the line indicates which conda environment is currently active: the base environment. Each environment will have its own version of Python, with its own packages. Conda takes care of all dependencies and so on.\nVERY IMPORTANTLY, DO NOT INSTALL THINGS ON THE BASE ENVIRONMENT. It’s a sure way to make things more confusing for yourself. Anything you do should have its own environment. You should never be doing any work on your base environment: it’s the environment conda uses to run itself, so a conflict or problem there can break your entire conda installation.\nSo first of all, let’s try to create a new, “clean” environment. Type:\nconda env create -n bestpractices\nThis will create a new environment named “bestpractices” (-n is shorthand for --name). Let’s activate it, so we can use it. Type:\nconda activate bestpractices\nThis should change the beginning of your prompt to (bestpractices).\nAlways remember to activate the environment you want to use or make changes to!\nTry running python. It probably didn’t work. Why?\nLet’s deactivate this empty environment and delete it. To deactivate, type:\nconda deactivate\nAnd then, to delete the environment use:\nconda env remove -n bestpractices\nTry running conda env list and you will see you have no environments other than base.\nNow let’s try to create an environment with Python. Type:\nconda env create -n bestpractices python=3.13\n(or pick a version of your choice). Activate your new environment as before and try:\npython --version\nYou can also try which python, depending on your platform. This should show you a path within the newly created environment, under your user, in a miniforge directory, rather than in a system location like /bin or /usr/bin.\nNow try running python to launch the interpreter and then import numpy as np. This should fail, but why?\n\n\n\n\n\nWith a conda environment active, you have two options for installing Python packages:\n\nUsing conda itself: conda install numpy\nThis will install the package from the active conda channel, in this case conda-forge. It can be used to install packages that go beyond just Python! For a listing, see: https://conda-forge.org/packages/\nUsing the Python package installer pip: pip install numpy\nThis will install Python packages from PyPI (the Python Package Index).\n\nGenerally, you can use either method, but mixing them can sometimes lead to conflicts. If you do use both, try to install what you need using conda first and then use pip for packages that are not available via conda.\nTry running step 7 from above again. It should work now.\nNow let’s deactivate and delete this environment, as before using conda deactivate and conda env remove -n bestpractices.\nThis time, we will try to create an environment using the environment.yml in this repository. Navigate to where you cloned the workshop repository in your terminal and have a look at the contents of environment.yml.\nNow, run:\nconda env create -f environment.yml\nThis will create a new environment named bestpractices_final with all the packages specified in the environment.yml file. Activating the new environment as before, using conda activate bestpractices_final (why is this the name?) and try running python --version. Try running conda list to see all the installed packages. Everything listed in the environment.yml should be there, along with all dependencies.\nNote: The pip equivalent, once you have an activated virtual environment with a Python interpreter, is:\npip install -r requirements.txt\nThis will install all packages listed in a requirements.txt file from PyPI.\nA final tip: append --dry-run to your conda commands to see what they will do without actually making any changes.\n\n\n\n\nBeyond using virtual environments, here are some best practices when it comes to managing dependencies for your projects:\n\nKeep track of every package for every module you import from in your code! Maintain either an environment.yml (for conda) or a requirements.txt file (for pip) that lists all of the dependencies needed to run your code.\nFor reproducibility, it’s a good idea to specify (pin) the specific versions of your dependencies you are using in your environment files.\nFor portability and ability to incorporate your code into other projects, avoid pinning versions strictly: pin to the minimum version required for the functionality you need.\nIn addition to your environment file, for maximal reproducibility, consider generating a lock file, which specifies the exact versions of all packages and their dependencies in your environment. This can be done with conda using conda-lock or with pip using pip-tools. (Additionally, both uv and pixi automatically generate lock files.)\nIf you will be sharing your code or expect others to run it, consider making a Python package. Check out the pyOpenSci Python Package Guide for more details on how to do that!",
    "crumbs": [
      "Virtual Environments"
    ]
  },
  {
    "objectID": "conda.html#installing-conda-if-you-dont-have-it-already",
    "href": "conda.html#installing-conda-if-you-dont-have-it-already",
    "title": "Working with virtual environments and conda",
    "section": "",
    "text": "There are several distributions of conda and it can pull packages from many different sources called “channels”. In this workshop we will use the conda-forge distribution called “miniforge” (or “miniforge3”).\n\nGo to the conda-forge downloads page: https://conda-forge.org/download/.\n\nDownload the installer for your operating system (Mac, Windows, Linux) and architecture (x86_64/amd64 or arm64/aarch64) and follow the installation instructions on that page.\n\nNote: When prompted whether to “automatically initialize conda”, I recommend saying “yes”. And on Windows, I recommend checking the options to “Create start menu shortcuts” and “Add Miniforge3 to my PATH environment variable”.\n\nOpen a NEW terminal window. If you see (base) at the beginning of your prompt, you have installed it successfully. You can also run conda --version and see if you get a version output.",
    "crumbs": [
      "Virtual Environments"
    ]
  },
  {
    "objectID": "conda.html#environments-how-do-they-work",
    "href": "conda.html#environments-how-do-they-work",
    "title": "Working with virtual environments and conda",
    "section": "",
    "text": "The (base) at the beginning of the line indicates which conda environment is currently active: the base environment. Each environment will have its own version of Python, with its own packages. Conda takes care of all dependencies and so on.\nVERY IMPORTANTLY, DO NOT INSTALL THINGS ON THE BASE ENVIRONMENT. It’s a sure way to make things more confusing for yourself. Anything you do should have its own environment. You should never be doing any work on your base environment: it’s the environment conda uses to run itself, so a conflict or problem there can break your entire conda installation.\nSo first of all, let’s try to create a new, “clean” environment. Type:\nconda env create -n bestpractices\nThis will create a new environment named “bestpractices” (-n is shorthand for --name). Let’s activate it, so we can use it. Type:\nconda activate bestpractices\nThis should change the beginning of your prompt to (bestpractices).\nAlways remember to activate the environment you want to use or make changes to!\nTry running python. It probably didn’t work. Why?\nLet’s deactivate this empty environment and delete it. To deactivate, type:\nconda deactivate\nAnd then, to delete the environment use:\nconda env remove -n bestpractices\nTry running conda env list and you will see you have no environments other than base.\nNow let’s try to create an environment with Python. Type:\nconda env create -n bestpractices python=3.13\n(or pick a version of your choice). Activate your new environment as before and try:\npython --version\nYou can also try which python, depending on your platform. This should show you a path within the newly created environment, under your user, in a miniforge directory, rather than in a system location like /bin or /usr/bin.\nNow try running python to launch the interpreter and then import numpy as np. This should fail, but why?",
    "crumbs": [
      "Virtual Environments"
    ]
  },
  {
    "objectID": "conda.html#installing-packages-in-your-environment",
    "href": "conda.html#installing-packages-in-your-environment",
    "title": "Working with virtual environments and conda",
    "section": "",
    "text": "With a conda environment active, you have two options for installing Python packages:\n\nUsing conda itself: conda install numpy\nThis will install the package from the active conda channel, in this case conda-forge. It can be used to install packages that go beyond just Python! For a listing, see: https://conda-forge.org/packages/\nUsing the Python package installer pip: pip install numpy\nThis will install Python packages from PyPI (the Python Package Index).\n\nGenerally, you can use either method, but mixing them can sometimes lead to conflicts. If you do use both, try to install what you need using conda first and then use pip for packages that are not available via conda.\nTry running step 7 from above again. It should work now.\nNow let’s deactivate and delete this environment, as before using conda deactivate and conda env remove -n bestpractices.\nThis time, we will try to create an environment using the environment.yml in this repository. Navigate to where you cloned the workshop repository in your terminal and have a look at the contents of environment.yml.\nNow, run:\nconda env create -f environment.yml\nThis will create a new environment named bestpractices_final with all the packages specified in the environment.yml file. Activating the new environment as before, using conda activate bestpractices_final (why is this the name?) and try running python --version. Try running conda list to see all the installed packages. Everything listed in the environment.yml should be there, along with all dependencies.\nNote: The pip equivalent, once you have an activated virtual environment with a Python interpreter, is:\npip install -r requirements.txt\nThis will install all packages listed in a requirements.txt file from PyPI.\nA final tip: append --dry-run to your conda commands to see what they will do without actually making any changes.",
    "crumbs": [
      "Virtual Environments"
    ]
  },
  {
    "objectID": "conda.html#some-advice-on-projects-and-dependencies",
    "href": "conda.html#some-advice-on-projects-and-dependencies",
    "title": "Working with virtual environments and conda",
    "section": "",
    "text": "Beyond using virtual environments, here are some best practices when it comes to managing dependencies for your projects:\n\nKeep track of every package for every module you import from in your code! Maintain either an environment.yml (for conda) or a requirements.txt file (for pip) that lists all of the dependencies needed to run your code.\nFor reproducibility, it’s a good idea to specify (pin) the specific versions of your dependencies you are using in your environment files.\nFor portability and ability to incorporate your code into other projects, avoid pinning versions strictly: pin to the minimum version required for the functionality you need.\nIn addition to your environment file, for maximal reproducibility, consider generating a lock file, which specifies the exact versions of all packages and their dependencies in your environment. This can be done with conda using conda-lock or with pip using pip-tools. (Additionally, both uv and pixi automatically generate lock files.)\nIf you will be sharing your code or expect others to run it, consider making a Python package. Check out the pyOpenSci Python Package Guide for more details on how to do that!",
    "crumbs": [
      "Virtual Environments"
    ]
  }
]